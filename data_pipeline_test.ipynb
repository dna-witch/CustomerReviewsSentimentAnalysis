{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shak-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shak-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shak-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_pipeline import ETL_Pipeline\n",
    "# pip install scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data pipeline\n",
    "etl = ETL_Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Text data extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "etl.extract(data_path='data/amazon_movie_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_title</th>\n",
       "      <th>text</th>\n",
       "      <th>images_x</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>...</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images_y</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>bought_together</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Amazon, please buy the show! I'm hooked!</td>\n",
       "      <td>[]</td>\n",
       "      <td>B013488XFS</td>\n",
       "      <td>B013488XFS</td>\n",
       "      <td>AGGZ357AO26RQZVRLGU4D4N52DZQ</td>\n",
       "      <td>1440385637000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>['IMDb 8.1', '2017', '10 episodes', 'X-Ray', '...</td>\n",
       "      <td>['A\\xa0con man (Giovanni Ribisi) on the run fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'360w': 'https://images-na.ssl-images-amazon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Suspense', 'Drama']</td>\n",
       "      <td>{'Content advisory': ['Nudity', 'violence', 's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>My Kiddos LOVE this show!!</td>\n",
       "      <td>[]</td>\n",
       "      <td>B00CB6VTDS</td>\n",
       "      <td>B00CB6VTDS</td>\n",
       "      <td>AGKASBHYZPGTEPO6LWZPVJWB2BVA</td>\n",
       "      <td>1461100610000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>['2014', '13 episodes', 'X-Ray', 'ALL']</td>\n",
       "      <td>['Follow the adventures of Arty and his sideki...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'360w': 'https://images-na.ssl-images-amazon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Kids']</td>\n",
       "      <td>{'Audio languages': ['English Dialogue Boost: ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Some decent moments...but...</td>\n",
       "      <td>Annabella Sciorra did her character justice wi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B096Z8Z3R6</td>\n",
       "      <td>B096Z8Z3R6</td>\n",
       "      <td>AG2L7H23R5LLKDKLBEF2Q3L2MVDA</td>\n",
       "      <td>1646271834582</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'360w': 'https://images-na.ssl-images-amazon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Content advisory': ['Violence', 'substance u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Decent Depiction of Lower-Functioning Autism, ...</td>\n",
       "      <td>...there should be more of a range of characte...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B09M14D9FZ</td>\n",
       "      <td>B09M14D9FZ</td>\n",
       "      <td>AG2L7H23R5LLKDKLBEF2Q3L2MVDA</td>\n",
       "      <td>1645937761864</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'360w': 'https://images-na.ssl-images-amazon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Content advisory': ['Violence', 'alcohol use...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>What Love Is...</td>\n",
       "      <td>...isn't always how you expect it to be, but w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B001H1SVZC</td>\n",
       "      <td>B001H1SVZC</td>\n",
       "      <td>AG2L7H23R5LLKDKLBEF2Q3L2MVDA</td>\n",
       "      <td>1590639227074</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'360w': 'https://images-na.ssl-images-amazon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Subtitles': ['None available'], 'Directors':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rating                                       review_title  \\\n",
       "0           0     5.0                                         Five Stars   \n",
       "1           1     5.0                                         Five Stars   \n",
       "2           2     3.0                       Some decent moments...but...   \n",
       "3           3     4.0  Decent Depiction of Lower-Functioning Autism, ...   \n",
       "4           4     5.0                                    What Love Is...   \n",
       "\n",
       "                                                text images_x        asin  \\\n",
       "0           Amazon, please buy the show! I'm hooked!       []  B013488XFS   \n",
       "1                         My Kiddos LOVE this show!!       []  B00CB6VTDS   \n",
       "2  Annabella Sciorra did her character justice wi...       []  B096Z8Z3R6   \n",
       "3  ...there should be more of a range of characte...       []  B09M14D9FZ   \n",
       "4  ...isn't always how you expect it to be, but w...       []  B001H1SVZC   \n",
       "\n",
       "  parent_asin                       user_id      timestamp  helpful_vote  ...  \\\n",
       "0  B013488XFS  AGGZ357AO26RQZVRLGU4D4N52DZQ  1440385637000             0  ...   \n",
       "1  B00CB6VTDS  AGKASBHYZPGTEPO6LWZPVJWB2BVA  1461100610000             0  ...   \n",
       "2  B096Z8Z3R6  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1646271834582             0  ...   \n",
       "3  B09M14D9FZ  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1645937761864             1  ...   \n",
       "4  B001H1SVZC  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1590639227074             0  ...   \n",
       "\n",
       "                                            features  \\\n",
       "0  ['IMDb 8.1', '2017', '10 episodes', 'X-Ray', '...   \n",
       "1            ['2014', '13 episodes', 'X-Ray', 'ALL']   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         description price  \\\n",
       "0  ['A\\xa0con man (Giovanni Ribisi) on the run fr...   NaN   \n",
       "1  ['Follow the adventures of Arty and his sideki...   NaN   \n",
       "2                                                NaN   NaN   \n",
       "3                                                NaN   NaN   \n",
       "4                                                NaN   NaN   \n",
       "\n",
       "                                            images_y  videos  store  \\\n",
       "0  [{'360w': 'https://images-na.ssl-images-amazon...      []    NaN   \n",
       "1  [{'360w': 'https://images-na.ssl-images-amazon...      []    NaN   \n",
       "2  [{'360w': 'https://images-na.ssl-images-amazon...      []    NaN   \n",
       "3  [{'360w': 'https://images-na.ssl-images-amazon...      []    NaN   \n",
       "4  [{'360w': 'https://images-na.ssl-images-amazon...      []    NaN   \n",
       "\n",
       "              categories                                            details  \\\n",
       "0  ['Suspense', 'Drama']  {'Content advisory': ['Nudity', 'violence', 's...   \n",
       "1               ['Kids']  {'Audio languages': ['English Dialogue Boost: ...   \n",
       "2                    NaN  {'Content advisory': ['Violence', 'substance u...   \n",
       "3                    NaN  {'Content advisory': ['Violence', 'alcohol use...   \n",
       "4                    NaN  {'Subtitles': ['None available'], 'Directors':...   \n",
       "\n",
       "  bought_together author  \n",
       "0             NaN    NaN  \n",
       "1             NaN    NaN  \n",
       "2             NaN    NaN  \n",
       "3             NaN    NaN  \n",
       "4             NaN    NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             int64\n",
       "rating               float64\n",
       "review_title          object\n",
       "text                  object\n",
       "images_x              object\n",
       "asin                  object\n",
       "parent_asin           object\n",
       "user_id               object\n",
       "timestamp              int64\n",
       "helpful_vote           int64\n",
       "verified_purchase       bool\n",
       "main_category         object\n",
       "movie_title           object\n",
       "subtitle              object\n",
       "average_rating       float64\n",
       "rating_number        float64\n",
       "features              object\n",
       "description           object\n",
       "price                 object\n",
       "images_y              object\n",
       "videos                object\n",
       "store                 object\n",
       "categories            object\n",
       "details               object\n",
       "bought_together      float64\n",
       "author                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl.data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first step in my Sentiment Analysis project using the Amazon Reviews Dataset. The preprocessing steps for the text data are:\n",
    "+ Removal of HTML tags\n",
    "+ Lowercasing the words\n",
    "+ Expanding contractions\n",
    "+ Removing special characters\n",
    "+ Removing stopwords\n",
    "+ Lemmatization\n",
    "+ Tokenization\n",
    "\n",
    "Some additional preprocessing steps that I have not yet implemented, but which may be useful, are:\n",
    "+ Removal of URLs\n",
    "+ Removal of extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing lets us clean, normalize, and transform raw text data into a format that is suitable for NLP tasks, such as sentiment analysis. \n",
    "\n",
    "For example, converting text to lowercase and expanding contractions ensures consistency within the data. Removing special characters, punctuation, and irrelevant symbols reduces noise in the text, making it easier to analyze. Stopwords are common words like \"the\", \"is\", or \"and\", which are very frequently found throughout text, but carry little semantic meaning. By removing stopwords, we are reducing the dimensionality of the text data and also retaining only the words that provide the most information about the meaning of the text. This will improve the efficiency and accuracy of the NLP model for sentiment analysis. \n",
    "\n",
    "Lemmatization is a technique to reduce words to their root forms (ex. running -> run). This helps consolidate variations of words, reducing the complexity of the resulting 'vocabulary' and improving model generalization.\n",
    "\n",
    "One technique that I would have employed if I went back to try and improve my model's accuracy, would be to generate n-grams. This could help capture context better by capturing negation (ex. \"not good\" vs \"good\"), and giving my model a better ability to recognize nuance in language. I, along with many of my classmates, noticed that some words show up with high frequency in all the rating brackets, such as \"good\", and \"like\". For lower rating reviews, the full phrases are most likely \"not good\" or \"did not like\", but this context was not captured by my model. If I'd included bigrams or trigrams, I could have captured these patterns more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data preprocessed successfully!\n",
      "Text data tokenized and lemmatized successfully!\n",
      "Vocabulary created successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(CountVectorizer(),\n",
       " 0                       [amazon, please, buy, show, hooked]\n",
       " 1                                      [kiddos, love, show]\n",
       " 2         [annabella, sciorra, character, justice, portr...\n",
       " 3         [range, character, highfunctioning, autism, ja...\n",
       " 4         [always, expect, know, movie, deep, struggle, ...\n",
       "                                 ...                        \n",
       " 999995                                  [go, wrong, martin]\n",
       " 999996                                  [go, wrong, martin]\n",
       " 999997    [good, pace, action, good, character, plot, bi...\n",
       " 999998    [watched, th, whole, thing, one, criterion, ac...\n",
       " 999999    [start, somewhat, interesting, fade, episode, ...\n",
       " Name: text, Length: 1000000, dtype: object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `encode()` function takes a method argument with options 'bow' (Bag of Words), 'tf-idf', and 'word2vec', with Word2Vec being the default encoding option.\n",
    "Here, I am testing the 'bow' encoding method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding is the process of converting the textual data into a numerical format that machine learning algorithms can understand and process. This is achieved by transforming words, phrases, or documents into numerical vectors. My `encode()` method can do this via three separate techniques.  \n",
    "\n",
    "Bag of Words represents text by counting the frequency of each word in the document and then creating a sparse matrix where each row corresponds to a document, and each column to a unique word in the corpus. The value of each cell is the frequency of the corresponding word in the document. \n",
    "\n",
    "TF-IDF also represents text by considering the frequency of the words, but it also weighs each word based on its importance in the corpus, by calculating the term frequency (TF) and the inverse document frequency (IDF), which measures how unique or rare that word is across documents in the corpus. \n",
    "\n",
    "Word2Vec represents words in a continuous vector space where semantically similar words are mapped to nearby points. This captures contextual relationships between words by training a neural net to predict a target word based on its surrounding words, or vice versa. This type of embedding better preserves semantic relationships between words so that the model can better capture meaning and context compared to BoW or TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Encoding completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1000000x393022 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20314046 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl.encode(method='bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
